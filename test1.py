import torch
import sys

print("=" * 60)
print("RTX 4060 GPUç¯å¢ƒéªŒè¯")
print("=" * 60)

print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print("ğŸ‰ RTX 4060 GPUé…ç½®æˆåŠŸï¼")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        gpu_props = torch.cuda.get_device_properties(i)
        print(f"\nğŸ® GPU {i}: {gpu_props.name}")
        print(f"  æ˜¾å­˜: {gpu_props.total_memory / 1024**3:.1f} GB")
        print(f"  è®¡ç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
        print(f"  å¤šå¤„ç†å™¨: {gpu_props.multi_processor_count}")
    
    # æ€§èƒ½æµ‹è¯•
    print("\nğŸ§ª æ€§èƒ½æµ‹è¯•...")
    
    # æµ‹è¯•1: çŸ©é˜µä¹˜æ³•
    size = 4096
    a = torch.randn(size, size).cuda()
    b = torch.randn(size, size).cuda()
    
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    start.record()
    c = torch.matmul(a, b)
    end.record()
    
    torch.cuda.synchronize()
    elapsed_time = start.elapsed_time(end)
    
    print(f"âœ… çŸ©é˜µä¹˜æ³• ({size}x{size}): {elapsed_time:.2f} æ¯«ç§’")
    print(f"âœ… æµ‹è¯•å¼ é‡è®¾å¤‡: {c.device}")
    
    # å†…å­˜æµ‹è¯•
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    print(f"ğŸ’¾ æ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB / {reserved:.2f} GB")
    
    print("\nğŸš€ RTX 4060éå¸¸é€‚åˆTTSä»»åŠ¡ï¼")
    print("é¢„è®¡æ€§èƒ½:")
    print("  â€¢ TTSæ¨ç†é€Ÿåº¦: 0.1-0.3ç§’/å¥")
    print("  â€¢ æ”¯æŒæ‰¹é‡å¤„ç†")
    print("  â€¢ å®æ—¶è¯­éŸ³åˆæˆ")
    
else:
    print("âŒ GPUé…ç½®å¤±è´¥")
    print("è¯·æ£€æŸ¥:")
    print("1. æ˜¯å¦åœ¨tts_gpuç¯å¢ƒä¸­")
    print("2. PyTorchæ˜¯å¦å®‰è£…äº†CUDA 12.6ç‰ˆæœ¬")

print("=" * 60)